#!/usr/bin/perl
#
# Script to manage my Odroid software mirror and associated torrent
# files.
#

use strict;
use warnings;

use constant WEBROOT => "/var/www/torrents/dn.odroid.com";
use constant SRCROOT => "/home/torrent/torrents/dn.odroid.com";

use constant WGETPIDFILE => "/home/torrent/.odroid_wget_pid";

use constant ODROIDROOTURL => "http://dn.odroid.com";

use constant XATTRTAG    => "otorrent.";
use constant XATTRBLOCK  => XATTRTAG . "sourceinfo.blocked";

my @trackers = (
  "udp://tracker.publicbt.com:80/announce",
  "http://tracker.publicbt.com:80/announce",
  "udp://tracker.openbittorrent.com:80/announce",
  "http://tracker.openbittorrent.com:80/announce",
  "http://www.torrent-downloads.to:2710/announce",
  "http://denis.stalker.h3q.com:6969/announce",
  "udp://denis.stalker.h3q.com:6969/announce",
  "udp://fr33domtracker.h33t.com:3310/announce",
  "http://fr33dom.h33t.com:3310/announce",
  "http://www.sumotracker.com/announce",
  "udp://tracker.istole.it:80",
  "udp://tracker.ccc.de:80",
  "http://open.tracker.thepiratebay.org/announce",
  # the following site apparently requires files be registered (ie,
  # not an open tracker):
  # "http://linuxtracker.org:2710/00000000000000000000000000000000/announce",
);

# cache file is a list of files on the main http server. Each line is
# a null-separated list of the filename and the attributes whose
# indexes are listed below
use constant CACHEFILE => "/home/torrent/.otorrent-cache";
use constant CMTIME    => 1;
use constant CSIZE     => 2;

# some useful stat-related constants
use constant SMODE     => 2;
use constant SSIZE     => 7;
use constant SMTIME    => 9;

# external programs follow
use constant BTMAKEMETAFILE => '/usr/bin/btmakemetafile';
use constant BTSHOWMETAFILE => '/usr/bin/btshowmetainfo';
use constant TRANSMISSION   => '/usr/bin/transmission-remote';

# if there's no password file for transmission, we will fall back to
# trying the default password
use constant XMISSIONPASSFILE => '/home/torrent/.transmission-netrc';
use constant XMISSIONDEFPASS  => 'transmission:transmission';

# set up base transmission command using netrc or default password
my @base_xmission_command = (TRANSMISSION,
			     (-f XMISSIONPASSFILE) ?
			     ("--netrc", XMISSIONPASSFILE):
			     ("--auth", XMISSIONDEFPASS)
			    );

# list of logins on hosts that will be serving the torrents
my @slaves = (
	      'torrent@declanmalone.com',
	      'torrent@euler',
	      'torrent@euclid',
	      'torrent@ks362632.kimsufi.com',
	      'torrent@hamilton', # not working, for some reason
);

# Required modules
use Cwd qw(abs_path);
use File::Path qw(make_path);
use File::Find;
use File::Copy;
use Fcntl qw(:DEFAULT :flock);
use Digest::MD5;

# Optional modules (flags are set if we have working module support)
my $xattr_support;
my $spider_support;
my $rpc_support;

# Test for extended attribute support. I'm leaning towards making
# xattr support mandatory, but since there are still some things that
# the script can do without it (and since the code below is useful)
# I'm leaving the checks in here.

BEGIN {
  $xattr_support = 0;
  if ( eval { require File::ExtAttr } ) {
    File::ExtAttr->import(':all');

    # The module is loaded now as if we'd done 'use File::ExtAttr',
    # but we have to check whether the filesystem actually supports
    # extended attributes. The man page for the module suggests that
    # the getfattr routine emits a warning in cases other than the
    # file not having a particular attribute. I'm assuming that this
    # includes the case where extended attributes aren't supported on
    # that file. (update: getfattr doesn't seem to work like this, so
    # I'm using setfattr instead)

    # we'll test if the source (seed) directory can have attributes,
    # but only if that dir exists.
    unless (-d SRCROOT) {
      warn "$0: Seed dir doesn't exist! This script needs one!\n";
    } else {

      eval {
	# promote warning into full die. Sets $@
	local $SIG{__WARN__} = sub { die $_[0] };

	# apparently getfattr (or the external attr command) doesn't
	# throw any warning, even on files like those in /proc or
	# /sys. That can't be right, so I'll use setfattr instead
	# getfattr ("/sys/block", "anything"); # no warning?!

	# my $testfile = "/proc/uptime"; # tested OK, raised warning
	my $testfile = SRCROOT;
	if (setfattr ($testfile, "user.testxattr", "any value")) {
	  delfattr($testfile,"user.testxattr");
	} else {
	  # this warn is promoted to a die thanks to our sig handler
	  warn "Unable to set test attribute on $testfile\n";
	}
      };

      # eval sets $@ if the eval block died
      if ($@) {
	# my $device = (stat SRCROOT)[0]; # != device *file*; no use
	warn "$0: seed dir doesn't seem to support ext. attributes: $@\n";
      } else {
	# warn "$0: Ext. attribute supported! Results caching enabled\n";
	++$xattr_support;
      }
    }
  }
}


# I'll also check for the availability of the various modules needed
# for doing web crawling on the odroid site. Without them, we won't be
# able to crawl the site or mirror files from it.

BEGIN {
  $spider_support = 0;
  if ( eval { require LWP::Simple } ) {
    LWP::Simple->import;

    # add more custom tests if needed ...
    if (1) {
      ++$spider_support;
    }
  }

  # use HTML::LinkExtor instead of full-blown HTML::Parser
  if ( eval { require HTML::LinkExtor } ) {
    HTML::LinkExtor->import;

    # add more custom tests if needed ...
    if (1) {
      ++$spider_support;
    }
  }

  # LWP::Simple's get and mirror methods are too simple if we want to
  # be able to continue a download. For that we need LWP::UserAgent
  if ( eval { require LWP::UserAgent } ) {
    LWP::UserAgent->import();

    # add more custom tests if needed ...
    if (1) {
      ++$spider_support;
    }
  }

  # we need all modules; one is not enough
  $spider_support = 0 unless $spider_support == 3;
}

# While we can do mirroring using just the web modules, if we want to
# log on to other peer hosts to check which files they've mirrored or
# are hosting, we need to have the GRID::Machine module too. Since I'm
# most likely going to be running this "coverage" test on a single,
# separate machine that's authenticated to log into all the other
# hosts, I won't need to install this module on any other host.
# Incidentally, I could equally have used IPC::PerlSSH instead of
# GRID::Machine, but I chose the latter since it's newer and more
# fully-featured.

BEGIN {
  $rpc_support = 0;
  if ( eval { require GRID::Machine } ) {
    GRID::Machine->import;

    # add more custom tests if needed ...
    if (1) {
      ++$rpc_support;
    }
  }
}


# Helper function to grab all the otorrent.* extended attributes.
# returns a (possibly empty) hash of values
sub read_xattrs {

  my $file = shift;
  my $hashref = {};

  return $hashref unless $xattr_support;

  unless (-f $file) {
    warn "read_xattrs: $file doesn't exist!\n";
    return $hashref;
  }

  foreach (listfattr($file)) {
    # only look up odroid.* tags
    next unless XATTRTAG eq substr $_, $[, $[ + length XATTRTAG;
    my $value = getfattr($file, $_);
    # as a convenience, strip out odroid. prefix before saving
    substr ($_, $[, $[ + length XATTRTAG) = '';
    $hashref->{$_} = $value;
  }

  return $hashref;

}

sub write_xattrs {

  my $file = shift;
  my %hash = @_;

  return unless $xattr_support;

  unless (-f $file) {
    warn "write_xattrs: $file doesn't exist!\n";
    return;
  }

  # we put back in the "otorrent." prefix when writing
  while (my ($k,$v) = each %hash) {
    setfattr($file, XATTRTAG . "$k", $v);
  }

}

#
# As a convenience, I'm allowing the user to specify a .torrent file
# (existing in either the source directory or the www one) in place of
# a regular source file and vice-versa. Then each routine will choose
# the correct file to operate on:
#
# mirror:  source file
# check:   source file
# make:    source file    -> source torrent
# publish: source torrent -> www torrent
# start:   www torrent
#
# Since the code to do the translations is needed in several places,
# I'll factor it out here into a routine that can be used by each
# calling routine. It takes any of the three filenames (source file,
# source torrent or www torrent) and returns all three filenames. If
# one or other of the directories doesn't exist, we make it, too.

sub convert_filename {

  my ($file, $warn) = @_;

  $warn = 1 unless defined $warn;

  #unless ($expect eq "sourcefile" or $expect eq "sourcetorrent"
  #  or $expect eq "wwwtorrent") {
  #  die "convert_filename: invalid 'expect' arg $expect\n";
  #}

  #unless (-f $file) {
  #  warn "File $file doesn't exist!";
  #  return ();
  #}

  my ($srcfile, $srctorrent, $wwwtorrent, $root, $relative) = ();
  my $absfile = abs_path($file);

  # which dir is the input file in? This is using substr instead of
  # regular expressions since I'm not sure how constants are supposed
  # to with with regexp. Substr should be more efficient anyhow.
  foreach my $dir (SRCROOT, WEBROOT) {
    if ($dir eq substr $absfile, $[, $[ + length $dir) {
      $root     = $dir;
      $relative = substr $absfile,  $[ + length $dir;
    }
  }
  die "$file is not in source or www tree\n" unless defined ($root);

  unless ($relative =~ m|^(.*/)(.*?)(\.torrent)?$|) {
    die "regexp match failed (impossible?)";
  }
  my ($rel_dir, $rel_base, $rel_ext) = ($1,$2,$3);

  # do we need to call make_path? Optionally warn if we do.
  foreach my $dir (WEBROOT . $rel_dir, SRCROOT . $rel_dir) {
    unless (-d $dir) {
      warn "making dir $dir\n" if $warn;
      make_path ($dir, { mode => 0775 }) or die "Failed to make dir $dir\n";
    }
  }

  # mnemonic: list order reflects the order in which files are created
  return (SRCROOT . "$rel_dir$rel_base",
	  SRCROOT . "$rel_dir$rel_base.torrent",
	  WEBROOT . "$rel_dir$rel_base.torrent");
}

#
# Use btshowmetainfo to extract the info_hash from a .torrent file
#
sub extract_info_hash {

  # not callable by user, so we skip error checking
  my $file = shift;

  # using a forking recipe is needed to safeguard against possible
  # shell escapes with basic open(HANDLE, "prog args |") form.
  my ($pid,$info_hash)=();
  die "Can't fork: $!"
    unless defined($pid = open CHILD, "-|");
  if ($pid) {			# parent process
    while (<CHILD>) {
      #print "parsing: $_\n";
      if (/^info hash.*:\s(\S+)/) {
	$info_hash = $1;
	last;
      }
    }
    close CHILD or warn "child exited $?";
  } else {			# child process
    #warn "child about to exec " . BTSHOWMETAFILE. " $file\n";
    die "Can't exec " . BTSHOWMETAFILE . "\n" unless
      exec BTSHOWMETAFILE, $file;
  }
  return $info_hash;
}

# in a couple of places we need to check if an output/destination file
# exists so that we can avoid overwriting it unless we want to force
# the recreation. This routine handles that.
sub newer_target {

  my ($source,$target) = @_;
  return 0 unless -f $target;

  my $ttime = (stat(_))[SMTIME];

  # catch potential programming errors in calling routines
  die "newer_target: original file doesn't exist!" unless -f $source;

  my $stime = (stat(_))[SMTIME];

  return ($stime <= $ttime) ? 1 : 0;
}


# By default make_torrent_file will create the output torrent file in
# the same directory as the input file and apply a "Downloaded from
# <url>" comment. Both behaviours can be overridden with suitable
# options.
sub make_torrent_file {

  my %o = (
	   infile  => undef,	# required
	   comment => "default",# or set to undef -> no comment
	   outfile => undef,
	   force   => 0,
	   @_
	  );

  die "make_torrent_file: needs an infile => value parameter"
    unless defined $o{infile};

  my $outfile;
  ($o{infile}, $outfile) = (convert_filename($o{infile}))[0,1];
  $o{outfile} = $outfile unless defined $o{outfile};

  die "make_torrent_file: file '$o{infile} doesn't exist\n"
    unless -f $o{infile};

  # skip unforced re-creation or if the file is blocked.
  return if (newer_target($o{infile},$o{outfile}) and !$o{force});
  if (is_blocked($o{infile})) {
    warn "make_torrent_file: skipping blocked file $o{infile}\n";
    return;
  }

  # build up command based on default invocation plus input options
  my $primary_tracker = $trackers[0];
  my $alt_trackers    = join "|", @trackers;

  my @cmd = (BTMAKEMETAFILE, $primary_tracker, $o{infile},
	    "--announce_list", $alt_trackers);

  if (defined $o{comment}) {
    if ($o{comment} eq "default") {
      my $source_url = ODROIDROOTURL . substr $o{infile},$[ + length SRCROOT;
      $o{comment} = "Downloaded from $source_url";
    }
    push @cmd, "--comment", $o{comment};
  }

  push @cmd, "--target", $o{outfile};

  system @cmd;

  # if we have extended attribute support, save some metadata about
  # the original file using them. Note that I'm storing the mtime in
  # this way rather than simply changing the mtime of the torrent file
  # to match the original file since I want the torrent's mtime to
  # reflect when I created it.
  if ($xattr_support) {
    my ($size,$mtime) = (stat $o{infile})[7,9];

    # tags to put on output file
    my %otags = (
		 mtime => $mtime,
		 size  => $size,
		);

    # scan the original (input) file for any stored attributes
    my $itags = read_xattrs($o{infile});

    #foreach (keys %$itags) {
    #  print "input tag $_ => '" . $itags->{$_} . "'\n";
    #}

    # copy the md5 hash if it's not out of date (ie, file isn't newer)
    if (exists $itags->{"md5.hash"}) {
      if (exists $itags->{"md5.ts"}) {
	if ($mtime <= $itags->{"md5.ts"}) {
	  $otags{md5sum} = $itags->{"md5.hash"};
	} else {
	  warn "make_torrent_file: $o{infile}: stale md5 hash\n";
	}
      } else {
	warn "make_torrent_file: $o{infile}: missing md5 timestamp\n";
      }
    }

    # rename tags to be prefixed with "sourceinfo."
    foreach (keys %otags) {
      $otags{"sourceinfo.$_"} = $otags{$_};
      delete $otags{$_};
    }

    # saving the info hash of the torrent will be very useful later
    # when it comes to pausing or unseeding a torrent since
    # transmission-remote uses them to refer to torrents.

    my $info_hash;
    if (defined($info_hash = extract_info_hash($o{outfile}))) {
      #warn "got info_hash: $info_hash\n";
      $otags{"torrent.info_hash"} = $info_hash;

      # also save this info with the source file
      warn "make_torrent_file: failed to save info hash to $o{infile}\n"
	unless setfattr ($o{infile}, XATTRTAG . "torrent.info_hash",
			 $info_hash)

    } else {
      warn "make_torrent_file: couldn't find info hash in " .
	BTSHOWMETAFILE . " output\n";
    }

    write_xattrs($o{outfile}, %otags);
  }
}

# blocking a (source) file makes it impossible to make a torrent from
# it, mirror it or download/seed it using bittorrent. Here we have
# three routines for blocking, unblocking and checking whether a file
# is blocked. These work fine without xattr support, but are better
# with it enabled since the user is less likely to inadvertantly
# change or delete the extended attribute than they are to make it
# writable again.
sub block_file {

  my $file = shift;

  # value of the tag doesn't matter, but it stops File::ExtAttr carping
  setfattr($file, XATTRBLOCK,1) if $xattr_support;

  my $mode = (stat $file)[2];
  chmod (($mode & 0555), $file) or die "chmod failed: $!\n";
}

sub unblock_file {

  my $file = shift;

  # do the equivalent of chmod ug+w, ie group members can also write
  my $mode = (stat $file)[2];
  chmod (($mode | 0220), $file) or die "chmod failed: $!\n";

  delfattr($file, XATTRBLOCK) if $xattr_support;
}

sub is_blocked {

  my $file = shift;
  my $blocked = 0;

  if ($xattr_support and defined(getfattr($file, XATTRBLOCK))) {
    ++$blocked;
  }

  if (-w $file) {
    warn "$file has blocked xattr set, but it is writable!\n" if $blocked;
  } else {
    ++$blocked;
  }

  return $blocked;
}

# truncate stops torrenting a file, deletes its contents and blocks it
sub truncate_file {

  my $file = shift;

  my ($source,$storrent,$wtorrent) = convert_filename($file);

  if (-f $wtorrent) {
    stop_torrent($file);
  }

  # if the file doesn't exist, but we have a corresponding torrent
  # file, then create an empty file and block it (handy if we've
  # downloaded a load of torrent files from another site and want to
  # recreate a blank copy of each file and possibly selectively
  # unblock them later)
  unless (-f $source) {
    if (-f $storrent or -f $wtorrent) {
      open (FH,">",$source) or die "truncate_file: couldn't create file $!\n";
      close FH;
    }
  }

  # the truncate may fail if the file is already blocked, but there's
  # no point in making it writable first if we've only got to block it
  # again immediately after. We'll just warn instead of dying.
  warn "truncate failed: $!" unless truncate $source, 0;

  block_file($source);
}


# publish_torrent
sub publish_torrent {

  my %o = (
	   file  => undef,
	   force => 0,
	   @_
	  );

  die "publish_torrent needs a file => var option" unless defined $o{file};

  my ($original,$source,$target) = convert_filename($o{file});
  die "publish_torrent: source $source doesn't exist\n" unless -f $source;

  if (is_blocked($original)) {
    warn "publish_torrent: skipping blocked file $source\n";
    return;
  }

  return if (!$o{force} and newer_target($source,$target));

  copy($source,$target) or
    die "publish_torrent: copy $source, $target failed: $!\n";

  # copy all our extended attributes to the target file
  if ($xattr_support) {
    my $hashref = read_xattrs($source);
    write_xattrs($target,%$hashref);
  }
}

# The best torrent downloader I've used seems to be Transmission. The
# most important feature is that it can store the seed/download files
# in a separate place to the torrent, so it makes it easy to set up
# parallel directory structures for torrent files and the
# downloads. Also, it has a very fully-featured server/client mode of
# operation that makes it easy to program.

sub start_torrent {

  my $file = shift;

  my ($absfile, $original, $srctorrent, $download_dir) = ($file);

  ($original,$srctorrent,$absfile)=convert_filename($file);
  unless (-f $absfile) {
    warn "start_torrent: did not find $absfile\n";
    return;
  }
  if (-f $original and is_blocked($original)) {
    warn "start_torrent_file: skipping blocked file $original\n";
    return;
  }

  $download_dir = $srctorrent;
  $download_dir =~ s|(.*)/.*|$1|;

  my @command = (@base_xmission_command, "--download-dir", $download_dir);

  # it seems that we have to set the download path separately from
  # doing the add ... in older versions of transmission-remote anyway.
  system @command;

  # it can't hurt to include --download-dir option when adding too
  @command = (@base_xmission_command,
	      "--download-dir", $download_dir,
	      "--add", $absfile);

  #warn "Executing: " . (join " ", @command) . "\n";
  system @command;

}

# Due to the way transmission-remote works, we can't just give it a
# torrent name and ask it to stop it. Instead, we have to look it up
# by its torrent ID (see the list command here) or give it the
# torrent's info_hash. Here I try to use the stored info_hash first
# and if that fails, it re-extracts it from the torrent file.
sub stop_torrent {

  my $file = (convert_filename(shift))[2];

  die "No torrent file $file\n" unless -f $file;

  my $info_hash = undef;

  # method #1: xattr-based
  if ($xattr_support) {
    my $hashref = read_xattrs($file);

    if (exists $hashref->{"torrent.info_hash"}) {
      $info_hash = $hashref->{"torrent.info_hash"};
    }
  }

  # method #2: read the torrent file
  $info_hash = extract_info_hash($file) unless defined($info_hash);

  # Still no hash info? Nothing we can do except die ...
  die "Torrent $file doesn't have hash info; can't stop\n"
      unless defined($info_hash);

  my @command = (@base_xmission_command, "--torrent", $info_hash,
		 "--remove");
  system @command;
}

# Returns a list of all the files we're currently downloading or
# seeding.  Alternatively, if it's given an argument, it tells if
# we're currently torrenting that.
sub list_torrents {

}

# The way the md5sum files are stored on the odroid site is a little
# bit inconsistent. Sometimes there is a single filename.md5sum file
# corresponding to the original filename, but at other times there's
# one md5sum file that contains details of several files. At least all
# the files seem to have the same format internally (just the output
# of the md5sum program). If I want to automatically check the md5sums
# of downloads then I have to scan the entire directory in to check
# for .md5sum files and store the contents. I'm implementing a caching
# scheme that saves details of all md5sum files on a per-directory
# basis to avoid overheads of having to scan each directory multiple
# times for each file in it. I chose to cache all results rather than
# just the last directory since we might traverse the tree in
# depth-first mode and this would invalidate the cache for many
# accesses. Plus, there shouldn't be too many .md5sum files, so
# storing all of them shouldn't ever be a problem.
my %md5_cache = ();	# dir => file => md5sum

sub official_md5_info {
  my $file = shift;		# assumed to be absolute

  # regular expressions with "use" constants are messy, so make a copy
  my $src = SRCROOT;
  my ($dirpart,$filepart);
  if ($file =~ m|^$src/(.*)/(.*)|) {
    ($dirpart,$filepart) = ($1,$2);
  } else {
    die "offical_md5_info: invalid filename\n";
  }

  unless (exists $md5_cache{$dirpart}) {

    # Scan the directory looking for *.md5sum files

    # By setting up an empty hash for this dir here and now, it means
    # that we only scan it once (think of it as caching negative hits)
    $md5_cache{$dirpart} = {};

    # Perl's standard glob() and <*> operators are a bit lacking since
    # they fail on filenames that have spaces in them. Rather than use
    # File::Glob (bsd_glob) to get around that problem (and introduce
    # another dependency), I'll use the standard opendir and readdir
    # keywords instead.
    opendir (DIR, SRCROOT . "/$dirpart") || die;
    my ($f);
    while ($f = readdir DIR) {	# while(readdir DIR) needs perl >= 5.11.2

      # prepend full path so that all file tests and open work
      my $full = SRCROOT . "/$dirpart/$f";

      next unless -f $full;
      next unless $full =~ /\.md5sum$/i;

      if (open FH, "<", $full) {
	while (<FH>) {
	  chomp;
	  if (/^([0-9a-fA-f]+)\s+(.*)/) {
	    my ($thisfile,$thishash) = ($2,$1);
	    $md5_cache{$dirpart}->{$thisfile} = $thishash;
	  } else {
	    warn "offical_md5_info: $full: invalid .md5sum file format\n";
	    last;
	  }
	}
	close(FH);
      } else {
	warn "offical_md5_info: failed to open $f: $!\n";
      }
    }
    closedir DIR;
  }

  # now check if the file is in the cache
  if (exists $md5_cache{$dirpart}) {
    if (exists $md5_cache{$dirpart}->{$filepart}) {
      return  $md5_cache{$dirpart}->{$filepart};
    }
  }

  # For no match, return an empty string instead of undef. This makes
  # it easier for the calling program to use, since they can just
  # write if ($hash eq offical_md5_info($f)) { ... } and not generate
  # warnings about an "uninitialized value" in the comparison.
  return '';
}


# check_file attempts to detect errors in a file based on its file
# extension. If there is a corresponding md5 hash file it checks
# against that, too. Since all of these checks involve reading the
# whole file in (which can take a while) it caches the results by
# storing it as an extended attribute of the file (if we have extended
# attribute support).
my $xattr_support_warning = 0;
sub check_file {

  # warn once about not being able to cache results
  unless ($xattr_support_warning ++) {
    warn "No xattr support; check_file can't cache results\n"
      unless $xattr_support;
  }

  my $file = shift;

  my $check_method = undef;
  my @check_command = ();

  

}

# later on, I'd prefer to have a dynamic cgi script to display the
# contents of each directory, but for now I'll make do with creating a
# static HTML page in each directory with links to the dirs and files
# contained within. This obviously needs to be updated when new
# torrent files or subdirectories are placed in a the directory, so
# it's not as good as a dynamically-generated page.
sub reindex {



}

# Operates on source files and caches result in xattrs (if available)
sub md5_digest {

  my $file = shift;

  my ($hash,$ts) = ("-");
  if (-f $file) {
    # skip calculating hash if an up-to-date hash is stored in xattrs
    if ($xattr_support) {
      if (defined($ts = getfattr($file, XATTRTAG . "md5.ts"))) {
	$hash = getfattr($file, XATTRTAG . "md5.hash");
	return $hash if defined($hash) and $ts >= ((stat $file)[9]);
      }
    }

    my $ctx = Digest::MD5->new;
    $ts = time();	# time from start of hashing
    open FH, "<", $file or die "MD5: failed to open file: $!\n";
    binmode(FH);	# to be safe (though Linux shouldn't need it)
    $ctx->addfile(*FH); # needs a typeglob, apparently
    close(FH);
    $hash = $ctx->hexdigest;

    # save the hash and timestamp
    if ($xattr_support) {
      setfattr($file, XATTRTAG . "md5.hash", $hash) or die;
      setfattr($file, XATTRTAG . "md5.ts",   $ts)   or die;
    }
  }
  return $hash;
}

# Since I'm spreading my seeding across several hosts, I need a way of
# transporting extended attributes between hosts. My workflow so far
# has been to do all the initial mirroring and torrent creation on one
# machine, then to copy just the www directory to the other hosts
# (using rsync, which doesn't support transferring extended
# attributes) and from there starting those torrents and leaving them
# run so that they will continue to be seeded. After that, I truncate
# the original files. However, this means that the source files that I
# download and seed from the initial host (and the torrent files I
# rsync'd from there) won't have any of the extended attributes such
# as file hashes, original mtime, etc. The next suite of routines are
# intended to address that.


# Unlike all the other routines so far which either call
# convert_filename or take one of the outputs from that, dump_tags
# lets you specify any filename or directory name as an input. It will
# warning if the file/dir isn't in the source or www directory, but it
# will still continue. If a dir is specified, it automatically
# recurses into it.

sub dump_file_tags {

  my ($file) = shift;
  my $tags = read_xattrs($file);

  if (%$tags) {
    print "$file:\n";
    foreach my $tag (sort keys %$tags) {
      print "  " . XATTRTAG . "$tag $tags->{$tag}\n";
    }
    print "\n";
  }
}

# recursive directory scan
sub dump_dir_tags {

  my $dir = shift;

  unless (opendir DIR, $dir) {
    warn "dump_tags: Failed to open dir $dir: $!\n";
    return;
  }

  # do a breadth-first scan of this dir
  my @subdirs = ();
  local ($_);
  while ($_ = readdir DIR) {	# while(readdir DIR) needs perl >= 5.11.2
    next if /^\.\.?$/;

    my $this = "$dir/$_";

    next if -l $this;
    if (-d $this) {
      push @subdirs, $this;
    } elsif  (-f $this) {
      dump_file_tags($this)
    } else {
      warn "dump_tags: skipping special file $this\n";
    }
  }
  closedir DIR;

  # while/shift is slightly more memory-efficient than foreach
  while (my $subdir = shift @subdirs) {
    dump_dir_tags($subdir);
  }

}

# main entry point, which does some error checking and calls one of
# the two routines above.
sub dump_tags {
  my %o = (
	   file => undef,
	   absolute => 0,	# reports absolute filenames if set,
                                # defaults to relative
	   @_
	  );

  die "dump_tags: needs file => var argument\n" unless defined($o{file});

  # we need to convert to absolute filename to check if it's in the
  # source or www dir, regardless of whether we're printing relative
  # paths in our output
  my $absfile = abs_path($o{file});

  unless ((WEBROOT eq substr $absfile, $[, $[ + length WEBROOT) or
	  (SRCROOT eq substr $absfile, $[, $[ + length SRCROOT)) {
    warn "dump_tags: $o{file} isn't in source or www root\n";
  }

  $o{file} = $absfile if $o{absolute};

  if (-f $o{file}) {
    dump_file_tags($o{file});
  } elsif (-d $o{file}) {
    $o{file} =~ s|/$||;		# prettify dir name to stop double /
    dump_dir_tags($o{file});
  } else {
    die "dump_tags: no such file or directory $o{file}\n";
  }
}

# Since I've discovered that I don't have enough space on my VPS to
# mirror the entire odroid download site, I have to be able to
# selectively delete files. However, there are some provisos and other
# points of interest:
#
# * I still want to be able to make torrent files for the files, even
#   if I won't be able to seed them
#
# * Before deleting files (actually, truncating them and making them
#   unwriteable) I want to check that it's safe to do so (ie, the file
#   checks out OK and I have an up-to-date torrent file created, along
#   with hash information)
#
# * I need to be able to store all the full file's vital-statistics
#   (previous size, hash, mtime, download time and the file's info
#   hash) in the file's extended attributes
#
# * I need to prevent downloading the file again if it's marked as
#   deleted in this way. Likewise, I need to make sure that I don't
#   start seeding from the file
#
# * If I'm currently seeding, I need to stop seeding before deleting
#
# * It would be nice to have some way of passing information about
#   what files I'm actually seeding over to the web server.
#
# * I probably need a way to undo the delete, or at least provide a
#   mechanism so that if I get more storage later, I can unmark the
#   file and start downloading it again. In that case, it would be
#   nice to be able to keep previous vital-statistics around at least
#   until the new download finished. Renaming the file to something
#   else would work fine (say ".filename"), but obviously it adds more
#   complexity when it comes to checking the new download at this
#   point.
#
# * I have several files that I know didn't download properly. I want
#   to include the option of checking files locally (with check_file)
#   but it would be good to also be able to check things like file
#   sizes and mtimes against the originals.
#


#
# Download management
#
# Running an external wget command to make a (partial) mirror of the
# odroid site isn't an option for me since wget would try to
# re-download all the files that I want deleted (or die because it
# can't write to the zero-length files that I keep in their place). As
# a result, I need to replicate some of the features of wget here. The
# two main features I need are to scrape URLs from HTML documents (so
# I can recursively find what needs to be downloaded) and to be able
# to resume downloads. I also want to be able to get info about a URL,
# in particular its modification time.
#
# As far as download management itself goes, I'd like to have the
# option of running one or more downloads in the background. In order
# to avoid problems with concurrent access and the like, as well as
# having a "database" of which downloads are active, I'd again use
# extended attributes. Something like this, maybe:
#
# otorrent.status
#
# set to "complete", "download(ing)" or "ignore" (ie, deleted or don't
# want)
#
# otorrent.source
#
# set to "torrent" or "mirror" to indicate where we got/we want to get
# the file from. Later on, I could see myself wanting to download via
# torrents, but probably only on another machine.
#
# otorrent.sourceinfo.*
#
# various metadata about the source file (eg, torrent name in the case
# of torrents and mtime, size, last visited, etc. in the case of
# mirrored files)
#
# otorrent.*
#
# various other local metadata (eg, last download time, md5 digest)

#
#--------------------------------------------------------------------
#

#
# It's about time that I started looking into doing a bit of work
# needed to support mirroring files. The last updates to the odroid
# website threw a lot of data up there, and I'm struggling to keep
# track of which files I've downloaded to which machines and which
# files among them I've tested and made torrents of. A relatively easy
# solution comes in two parts: one part scans (crawls) the odroid main
# site and gathers information about all the files there (filenames,
# sizes and modification time), while the other logs into my hosts and
# at a minimum checks whether I have copies of those files in my www
# directory and/or source directory and checks whether those sources
# are incomplete, out of date or blank (the latter is most important,
# but checking for files that aren't blank, but don't match with the
# original is probably something I should do---at least it will let me
# distinguish between complete downloads and partial ones).
#
# I'm calling these two routines spider and coverage. The spider
# routine will definitely have to save its output to a file. To do it
# right, it will also have to do a graceful update to that file, I may
# also want to implement some sort of diff between the older
# file. This would allow me to quickly see what new files have
# appeared on the site since the last spider. I'd also like to be able
# to regenerate that report so the workflow will be something like
# spider   -> generate initial/new web crawl results
# newfiles -> show which files are new(*)
# do stuff -> test coverage, decide where to put files, start downloads,
#             etc.
# newfiles -> keep checking the report until all downloads are started
# coverage -> see download/torrent creation status
# chain    -> do publishing of completed torrents
# coverage -> keep checking until all torrents are published
#
# (*) it strikes me that maybe it's best for my newfiles report to be
# filtered through my coverage test to some degree. For a start, when
# I generate the first spider report, I'll have a big list of new
# files that I've already processed, so the first report won't me of
# any use to me. As a to-do list, it would be much more useful for me
# to work directly with the coverage list (and diffs thereof). So: a
# rethink...
#
# In a slightly-modified scheme, I would roll the web spidering in
# with checking the files on each of my machines. First, I check each
# dir on the master site and sort the entries before any
# processing. Then, because the entries are sorted, I can do a diff
# between the old report and the new one as I go. If I have any old
# report, and it tells me that I already have a mirror set up and its
# stats tally with what's on the current file on the main server, then
# I simply copy the entire record to the new spider file. The old
# report then acts like a cache, letting me avoid unnecessary
# accesses. If I have no information about the file, or if the cached
# information doesn't match with the main server, then I will log on
# to each of my storage hosts and check the stats of any matching file
# there. Finally, I'd write a record including details on the master
# file along with all the information gathered from the storage hosts.
#
# Even though this should be enough in most cases, I'd still want to
# force checks on what's on each storage host instead of just relying
# on cached data. Files on these hosts can change independently of any
# locally cached data, so I need to be able to update the
# cache. Although it's not a great name for it, I guess "coverage"
# will do since it does involve logging into my hosts to see how well
# they cover all the files on the main site. It differs from "spider"
# in that it doesn't contact the main odroid server at all, although
# it does have to update the master cache that the spider routine
# generates.
#
# (**) Another update: after writing the code to check for the
# availability of the web modules and GRID::Machine, it struck me that
# we need the web modules for mirroring, too, and it's only the
# coverage test that really needs GRID::Machine. Also, it would be
# very beneficial to keep a local cache of information about files on
# the odroid site even if we don't have (or can't generate) coverage
# information for other hosts. I'm actually getting a bit confused now
# about the best approach... maybe for mirroring we don't actually
# need any local cache at all? Hmmm... I guess it would still be
# useful if I want to generate a report comparing the current host's
# files against a slighty out-of-date list of master files since it's
# less wasteful than doing a full crawl of the odroid site each time.
#
# So, I've decided to keep the cache and use it for both mirroring and
# coverage tests. That still leaves the question about whether to
# couple crawling and (a possibly local) coverage test. I think that
# the answer should probably be "yes". It does mean that it makes
# doing the coverage test is more complex, but if we do all the hard
# work in one place then it makes ancillary reports and mirroring much
# easier to handle. Decoupling is also good because, for example, we
# can generate the cache globally and in one place, and then write our
# mirroring or coverage update tasks as filters that selectively
# operate only on part of the file. They still have to update the file
# gracefully, which is a problem when compared to using a database,
# but at least I can factor out the code that does "in-place" updates.

# For my locking routines I distinguish between read and write locks,
# with write locks being used for long-running update tasks like
# running a spider on the odroid site, with read locks being used to
# ensure that shorter-running tasks that just need to slurp the file
# in and do something with it get to see a consistent version of the
# file, regardless of whether a writing process updates it mid-way
# through the slurp.  I use non-blocking locks so that I can give
# feedback and also make several attempts to get the lock before
# giving up. Also note that we don't attempt to lock the file itself,
# but lock file.READ or file.WRITE since the file itself may not exist
# when we want to lock it (ie, file.READ and file.WRITE are what are
# known as "sentinels").

sub getlock {
  my ($file, $type, $verbose, $backoff, $tries) = @_;

  die "getlock: need a filename argument\n" unless defined $file;

  # provide some default values
  if ($type eq "READ") {
    $backoff = 2 unless defined $backoff; # read locks are short-lived
    $tries   = 3 unless defined $tries;   # pretty arbitrary
  } elsif ($type eq "WRITE") {
    $backoff = 10 unless defined $backoff; # write locks last longer
    $tries   = 0 unless defined $tries;	   # "infinite" wait
  } else {
    die "getlock: type argument must be READ or WRITE\n";
  }

  # by default, don't be too noisy
  $verbose = 0 unless defined $verbose;

  my ($lockname,$fh) = ("$file.$type");
  open $fh, "+>> $lockname" or die "getlock: open $lockname failed: $!\n";

  my $attempts = 1;
  do {

    if (flock($fh, LOCK_EX|LOCK_NB)) {

      # Store our PID in the lock file so that we can produce useful
      # warning/error messages later
      truncate $fh, 0;
      seek $fh, 0, 0;
      local ($|) = 1;
      print $fh "$$\n";

      warn "getlock: got lock $lockname on try number $attempts\n"
	if $verbose;

      # It's up to the calling routine to close the filehandle (and
      # hence the lock) when they're finished with it. As a result we
      # don't have a subroutine for releasing the lock.
      return $fh;
    }

    # No success this time, so [warn and] sleep for a while
    --$tries;
    warn "getlock: failed to lock $lockname (try # $attempts, $tries left)\n"
      if $verbose;

    ++$attempts;
    sleep $backoff;

  } while($tries);

  # Fail somewhat gracefully
  if ($verbose) {
    my $pid = <$fh>;
    chomp $pid;
    warn "getlock: giving up locking $lockname (locked by PID $pid)\n";
  }
  close ($fh);			# don't rely on perl's garbage collection

  return undef;
}

# a quick conversion from bytes to kbytes, etc.
sub human_readable {
  my $n = shift;
  my $u;
  my @units = qw(KB MB GB TB PB);
  return "${n} B" if $n <1024;
  while ($u = shift @units) {
    $n /= 1024;
    if ($n < 1024) {
      return sprintf("%.2f ",$n) . $u;
    }
  }
  return sprintf("%.2f ",$n ) . "PB";
}

# wrapper around LWP::UserAgent to resume download
sub download_url {

  my %o=(
	 url => undef,
	 file => undef,
	 tries => 5,
	 verbose => undef,
	 @_
	);

  die "download_url needs an url => var parameter\n" unless defined($o{url});
  die "download_url needs a file => var parameter\n" unless defined($o{file});
  $o{verbose} = 0 unless defined($o{verbose});

  my $size = 0;

  for (1..$o{tries}) {

    # check file's size within loop, since it may change
    if (-f $o{file}) {
      $size = (stat(_))[SSIZE];
    }

    my ($ua,$rc);
    $ua = LWP::UserAgent->new;
    $rc =$ua->get($o{url},
		  ':content_file' => $o{file},
		  'Range' => "$size-");
    #    warn "return value $rc\n";
    my $code = $rc->code;
    if ($rc->is_success) {
      warn "OK   ($code ". $rc->message .") : $o{url}\n" if $o{verbose};
      return 1;			# return success
    }

    # warn about failure and try again
    warn "FAIL ($code ". $rc->message .") $o{url}\n" if $o{verbose};

    # we might want additional checks here to account for specific
    # error returns like 404 (not found) or 416 (range not
    # satisfiable) that are unrecoverable so that we don't end up
    # trying them repeatedly. For now I'll just bail on 404s.
    if ($code == 404) {
      warn "Giving up on unrecoverable $code error\n" if $o{verbose};
      return 0;
    }

  }

  # return failure
  return 0;
}


# mirror does not require any updates to our cache so we don't need to
# go get a write lock on it. We do need to get a read lock, though, so
# that our input file is in a consistent state even if a writer
# happens to replace the file while we're reading it.

# we have to call this something other than "mirror" due to a
# conflicting name exported by the LWP::Simple module.
sub do_mirror {

  my $rooturl = shift;		# not exactly a URI, but close enough
  $rooturl = ODROIDROOTURL unless defined($rooturl);

  die "Modules required to mirror are not available!\n"
    unless $spider_support;

  unless (ODROIDROOTURL eq substr $rooturl, $[, $[ + length ODROIDROOTURL) {
    die "do_mirror: expected URL starting with " . ODROIDROOTURL . "\n";
  }

  do_diff
    (
     # all our work is done in insert_a callback (default callbacks
     # and other options work fine for us)
     insert_a => sub {
       my $filerec = shift;
       my ($rel,$mtime,$size) = (split "\0", $filerec)[0,CMTIME,CSIZE];

       my $file = SRCROOT . $rel;
       my $url  = ODROIDROOTURL . $rel;

       #warn "$file:\n";
       if ($file =~ s|/$||) {
	 make_path($file, { mode => 0775 }) unless -d $file;
       } else {
	 # LWP::Simple's mirror routine checks mtimes, but since we've
	 # cached the value, we use that to avoid an unnecessary web
	 # access
	 my $need_update = 1;
	 my ($local_mtime,$local_size)=(0,0);
	 if (-f $file) {
	   ($local_mtime,$local_size) = (stat(_))[SMTIME,SSIZE];
	   if ($local_size == $size and $local_mtime >= $mtime) {
	     $need_update = 0;
	   }
	 }
	 return unless $need_update;

	 warn "Mirroring $rel (" . human_readable($size) . ", have ".
	   (sprintf("%.2f", ($local_size * 100/$size))) . "%)\n";
	 # return;

	 # LWP::Simple's mirror doesn't support ranges, so I'm not using it
	 if ("LWP::Simple" eq "preferred") {

	   my $rc = mirror($url,$file);

	   if (is_success($rc)) {
	     warn "$rc (success): $file\n";
	   } else {
	     warn "$rc (fail) $file: " . status_message($rc). "\n";
	   }
	 } else {

	   # use my sub that uses LWP::UserAgent instead
	   download_url(url => $url, file => $file) or warn "$url failed\n";
	 }
       }
     },
    );
}


# wrapper routine that
sub coverage {


}

#warn "xattr_support? $xattr_support\n";


# Rewriting the above pseudocode sections as actual code based on
# notes found here and elsewhere (my notebook). The salient points
# are:
#
# * do_diff will accept various callbacks in a style similar to
#   Algorithm::Diff
#
# * the spider routine will be the only routine capable of creating
#   the cache of remote web pages, but apart from examining file sizes
#   and modification times, it won't do any downloading apart from
#   parsing index.html files
#
# * The mirror and coverage routines will update the cache, with other
#   routines just reading it and reporting on its contents.
#
# * I'll include the option to slurp in the file if it's not being
#   used for updating.
#
# * Even routines (such report generators) that don't do "diffs" as
#   such or that don't insert any new lines into the cache will still
#   call do_diff.  They'll simply implement their functionality in
#   "insert_a" callbacks.
#
# * I abandoned the idea of converting either the do_diff or calling
#   programs using explicit continutation style, since callback
#   handlers can access shared state using the closure mechanism.

# In keeping with Algorithm::Diff, I'm using A to refer to the
# original list, with the B list (actually a reference to one) being
# provided by the calling program.
sub do_diff {

  # Need to predeclare $B so that we can implement our default consume
  # callback below. Perl doesn't let us refer to $o{list} from within
  # the definition of %o itself, so I refer to it indirectly using $B.
  my $B;
  my %o = (
	   # A default value of file => undef below would make the
	   # code more generic. I'm just going for convenience here.
	   file     => CACHEFILE,
	   update   => 0,
	   slurp    => 1,	# read A in in one go?
	   list     => [],	# empty B list

	   # default callbacks don't do much at all
	   init     => sub { },
	   key      => sub { my $k = shift; chomp $k; return $k },
	   insert_a => sub { },
	   insert_b => sub { },
	   matching => sub { },
	   consume  => sub { shift @$B },
	   @_
 	  );
  $B = $o{list};

  # Do file locking, opening and slurping based on options

  # We use callbacks to handle "consume" operation on the original
  # file so that we can abstract away differences between slurping the
  # file in one go and reading it line by line.

  my @A;
  my $consume_a;
  my $close_a;
  my ($lock,$ifh,$ofh);

  if ($o{update}) {

    # Updating means we have to get a write lock.
    unless (defined($lock = getlock($o{file}, "WRITE", 1))) {
      die "Failed to get write lock\n";
    }
    open ($ofh, ">", "$o{file}.new")
      or die "failed to open output file: $!\n";

  } else {

    # If not updating, a short-lived read lock will do. This doesn't
    # work very well if the slurp option isn't set, but I'll worry
    # about that later if I ever *don't* want to slurp (probably by
    # tweaking the read lock semantics)
    unless (defined($lock = getlock($o{file}, "READ", 1))) {
      die "Failed to get read lock\n";
    }
  }

  # We need to allow for the case where there the input file doesn't
  # exist. Rather than do something like opening /dev/null instead (so
  # we have a filehandle to work with when we're not slurping) I'll
  # silently switch to slurp mode.
  $o{slurp} = 1 unless -f $o{file};

  # default consume closure for A corresponds to slurping an empty or
  # nonexistent file
  $consume_a = sub { return undef };

  if (-f $o{file}) {

    # file exists, but can we open it?
    open ($ifh, "<", $o{file})
      or die "failed to open input file: $!\n";

    # set up closure depending on slurp setting
    if ($o{slurp}) {
      @A = map { chomp; $_ } (<$ifh>);
      close $ifh;
      $ifh = undef;		# so we don't try to close it later
      $consume_a = sub { shift @A };

      unless ($o{update}) {
	close $lock;
	$lock = undef;
      }
    } else {
      $consume_a = sub { my $l = <$ifh>; chomp $l; $l };
    }
  }

  # Now that initialisation is complete, send our output file handle
  # back to caller so they can print to it.
  $o{init}->($ofh);

  # define aliases for the more commonly-used callbacks (less unweildy
  # than the syntax in the line above!)
  my $key        = $o{key};
  my $insert_a   = $o{insert_a};
  my $insert_b   = $o{insert_b};
  my $matching   = $o{matching};
  my $consume_b  = $o{consume};

  my ($a,$b);
  my $cancel = undef;		# consume callback can notify us to
                                # cancel an update (eg, hit Ctrl-C)

  ($a,$b,$cancel) = (&$consume_a(), &$consume_b());

  while (defined($a) and defined($b) and !defined($cancel)) {
    my $tilt = (&$key($a) cmp $b);
    if ($tilt == -1) {		# $a < $b
      &$insert_a($a);
      $a = &$consume_a();
    } elsif ($tilt == 0) {	# $a = $b
      #warn "Match '$a' with '$b'\n";
      &$matching($a,$b);
      # advance in both lists
      ($a,$b,$cancel) = (&$consume_a(), &$consume_b());
    } elsif ($tilt == +1) {	# $a > $b
      &$insert_b($b);
      ($b,$cancel) = &$consume_b();
    }
  }
  while (defined($a) and !defined($cancel)) {
    &$insert_a($a);
    $a = &$consume_a();
  }
  while (defined($b) and !defined($cancel)) {
    &$insert_b($b);
    ($b,$cancel) = &$consume_b();
  }

  # various cleanups depending on whether we're updating, cancelling
  # or still have a (non-slurped) read or write lock
  if ($o{update} and defined($cancel)) {
    # roll back update
    close $ofh;
    unlink "$o{file}.new";
  } elsif ($o{update}) {
    close $ofh;
    rename "$o{file}", "$o{file}.old";
    rename "$o{file}.new", "$o{file}"
  }

  close $ifh  if defined($ifh);
  close $lock if defined($lock);
}

sub spider_read_dir {
  my $rel = shift;
  my $url = ODROIDROOTURL . $rel;

  #warn "Visiting '$rel'\n";

  my @links = ();
  my $content = get($url);
  die "Failed to download $url\n" unless defined($content);

  my $h = HTML::LinkExtor->new
    (
     sub {
       my($tag, %attr) = @_;
       return unless $tag =~ /^a$/i;
       my $href = $attr{href};
       return unless $href =~ s|^$url||;
       return if $href =~ m|^\?|;   # skip internal navigation links
       return if $href =~ m|^\.\.|; # skip parent link
       #warn " +link $rel$href\n";
       push @links, "$rel$href";
     }, $url);
  $h->parse($content);

  # read in this directory and extract a list of contents
  # for convention, we'll use "dir/" and "file"
  return (sort { $a cmp $b } @links);
}

sub spider {

  die "Modules required to spider are not available!\n"
    unless $spider_support;

  # we save memory by omitting the base ODROID site URL, which is
  # implicitly added whenever we actually try to download something.
  my $url = "";
  my @queue   = (spider_read_dir($url));

  # set up signal handler
  my $intr = undef;
  local ($SIG{INT}) = sub {
    $SIG{INT} = 'IGNORE';	# we will quit in our own time anyway
    warn "Got interrupt signal. Rolling back update\n";
    $intr = 1;
  };

  # output file handle sent back in init callback
  my ($fh);
  #$|++;

  # since callbacks for insert_b and matching both effectively do the
  # same work, I factor that out here.
  local *save_header_info = sub {

    my $rel = shift;
    my $url = ODROIDROOTURL . $rel;
    my $existing = shift || '';
    my @fields   = split("\0",$existing);

    if ($rel=~m|/$|) {
      print $fh "$rel\n";
    } else {
      my ($content_type, $document_length, $modified_time,
	  $expires, $server) = head($url);
      die "Failed to get header info for $url\n" unless defined($content_type);
      #warn "Adding file info for $rel\n";
      $fields[0]            = $rel;
      @fields[CMTIME,CSIZE] = ($modified_time,$document_length);
      print $fh join("\0",@fields) . "\n";
    }

  };

  do_diff(
	  update   => 1,
	  list     => \@queue,
	  # we get a callback after do_diff finishes setting up files
	  init     => sub { ($fh) = @_; warn "INIT callback\n"},
	  key      => sub {
	    my ($key) = @_;
	    chomp $key;
	    $key =~ s/\0.*//;    # strip extra cached data
	    return $key;
	  },
	  # warn about deleted records
	  insert_a => sub { warn "- " . (shift) . "\n"; },
	  insert_b => sub {
	    # this is a new dir/file
	    my $b = shift;
	    warn "+ $b\n";
	    save_header_info($b);
	  },
	  matching => sub {
	    # update an existing record (saving any existing fields)
	    my ($a,$b) = @_;
	    warn "= $b\n";
	    save_header_info($b,$a);
	  },
	  # consume does the same as shifting one element from the
	  # queue, but if it's a dir, we also look up the contents and
	  # prepend them. This should preserve sort ordering in the
	  # file so long as "foo/" is less than "foo/bar"
	  consume  => sub {
	    my $item = shift(@queue);
	    if (defined($item) and !defined($intr)) {
	      if ($item =~ m|.*/$|) {
		#unshift @queue, spider_read_dir("$item");
		unshift @queue, spider_read_dir("$url$item");
	      }
	    }
	    return ($item,$intr);
	  },
	 );
}

# crontask puts everything together in one pipeline (split out into
# remote and local parts for easier testing)
sub crontask_remote {

  die "crontask needs spider support!\n" unless $spider_support;

  spider;
  do_mirror;

}

sub crontask_local {

  my ($fh);
  do_diff
    (
     # We will update the cache to include any new hashes, check
     # status and the like.
     update   => 1,
     init     => sub { ($fh) = @_; },
     key      => sub {
       my ($key) = @_;
       chomp $key;
       $key =~ s/\0.*//;    # strip extra cached data
       return $key;
     },
     # All our work is done in insert_a callback.
     insert_a => sub {
       my $a = shift;
       chomp $a;		# TODO: delete useless chomps
       my @A = split("\0",$a);
       my $f = SRCROOT . $A[0];

       # skip directories (writing original record back out
       # unmodified)
       if ($a =~ m|/$|) {
	 print $fh "$a\n";
	 return;
       }

       # extract any records that we might use or update
       my ($cmtime,$csize) =
	 @A[CMTIME,CSIZE];


       # skip missing or blocked files
       goto cleanup unless -f $f;
       goto cleanup if is_blocked($f);

       # hash (skipped, as it's done in check step anyway)
       #md5_digest($f);

       # 
       unless (check_file($f)) {
	 warn "Failed check: $f\n";
	 goto cleanup;
       }

       # make
       make_torrent_file(infile => $f);

       # publish
       publish_torrent(file => $f);

       # start
       start_torrent($f);


       # write the record plus any updates back out to the cache file
     cleanup:
       @A[CMTIME,CSIZE] =
	 ($cmtime,$csize);
       print $fh join("\0",@A) . "\n";

     },
    );

  # rebuild index.html files

}


my $prog = $0; $prog =~ s|.*/||;
my $usage = <<EOT;
$prog: Manage ODROID torrent mirror
Usage:

$prog mirror (file|dir)

  Download (updated) file contents from ODROID site

$prog check (file[.torrent]|dir)

  Check integrity of files in local mirror

$prog make (file[.torrent]|dir)

  Create .torrent files

$prog publish (file[.torrent]|dir)

  Copy torrents into www directory

$prog chain (file[.torrent]|dir)

  Chains check, make and publish operations into one step

$prog start (file.torrent|dir)

  Join the torrent network to download or seed files

$prog (pause|unpause) (file.torrent|dir)

  Pause or unpause particular torrent files (needs xattr support)

$prog stop (file.torrent|dir)

  Stop torrenting files. Needs xattr support to look up the torrent's
  info hash. Otherwise, use the transmission-remote command directly.

$prog truncate (file)

  THIS DELETES THE FILE! USE WITH CAUTION! Truncate stops torrenting
  the file, truncates it to zero bytes, and blocks it (see below).

  The intended use is to spread the mirror over several hosts, each of
  which will act as seeders for some part of the overall mirror. By
  truncating the file rather than deleting it, we can still store
  metadata about the original file in its extended attributes.
  Meanwhile, it's assumed that you've already got a copy of the file
  stored on another host (or you don't want any copies of it at all).

$prog block (file)

  This makes a source file unwriteable and adds a mark that says it
  shouldn't be used to create a new torrent file, mirrored, or
  downloaded or shared via bittorrent.

$prog unblock (file)

  The opposite of the block command. If you had truncated the file,
  you need to use start or mirror to redownload the file, or get it by
  some other means (eg, a backup copy).

$prog scan (file|file.torrent)

  Given a file or torrent, scan shows which other files with the same
  name exist in the source or www directories.

$prog retag (file)

  Attempts to regenerate all the extended attributes attached to the
  source file and the associated torrent files. 

$prog list

  Shows a list of all current torrents

$prog xtracker

  Tell transmission to use our current tracker list for all active
  torrents. This lets you continue seeding even if the torrent files
  have been changed to include new tracker info.

$prog crontask

  Does the entire pipeline of tasks: spider, mirror, check, make,
  publish and start. Suitable for running periodically as a cron task.
  This honours any blocks on downloading and runs make and publish
  without the force option, so torrents won't be updated unless they
  changed on the master site. If a file fails any check then it won't
  be processed either.

EOT

# set umask to allow creation of files/dirs that users in our group
# can write to

umask 0002;


my $command = shift @ARGV;

die $usage unless defined $command;

if ($command eq "make") {

  make_torrent_file(infile => abs_path(shift @ARGV));

} elsif ($command eq "publish") {

  publish_torrent(file => shift @ARGV);

} elsif ($command eq "chain") {

  my $file = shift @ARGV;
  check_file($file);
  make_torrent_file(infile => $file);
  publish_torrent(file => $file);

} elsif ($command eq "start") {

  start_torrent(shift @ARGV);

} elsif ($command eq "stop") {

  stop_torrent(shift @ARGV);

} elsif ($command eq "block") {

  my $file = (convert_filename(shift @ARGV))[0];
  block_file($file);

} elsif ($command eq "unblock") {

  my $file = (convert_filename(shift @ARGV))[0];
  unblock_file($file);

} elsif ($command eq "blocked") {

  my $file = (convert_filename(shift @ARGV))[0];
  print "Blocked status: " . is_blocked($file) . "\n";

} elsif ($command eq "truncate") {

  truncate_file(shift @ARGV);

} elsif ($command eq "dumptags") {

  my ($file,$absolute) = @ARGV;
  if (defined $absolute) {
    dump_tags(file => $file, absolute => $absolute);
  } else {
    dump_tags(file => $file);
  }

} elsif ($command eq "list") {

  my @command = (@base_xmission_command, "-l");

  exec @command;

} elsif ($command eq "xtracker") {

  # apparently transmission-remote doesn't like adding duplicate
  # trackers, and neither does it have an option to delete all
  # trackers. As a result, I'll just run the add and suppress any
  # warnings.

  foreach (@trackers) {

    # next if /^udp:/i;

    my $cmd = (join " ", @base_xmission_command) .
      " -t all --tracker-add $_";

    `$cmd `;			# `$cmd 2>/dev/null`;
  }
} elsif ($command eq "scan" or $command eq "tconvert") {

  my ($src,$storr,$wtorr) = convert_filename(shift);
  if (defined $src) {
    print "source  : " . (-f $src   ? "yes" : "no ") .  " $src\n";
    print "storrent: " . (-f $storr ? "yes" : "no ") .  " $src\n";
    print "wtorrent: " . (-f $wtorr ? "yes" : "no ") .  " $src\n";
  } else {
    die "convert_filename failed to return anything\n";
  }


} elsif ($command eq "selfupdate") {

  # copy this script to the bin dir of each slave machine
  for my $m (@slaves) {
    `scp $0 $m:bin/`;
  }

} elsif ($command eq "websync") {

  # synchronise the torrent collection (in www dir) across each slave

  # first rsync all the files into our local webroot
  for my $m (@slaves) {
    warn "Receiving from $m\n";
    system ("rsync", "-avX", "$m:" . WEBROOT . "/", WEBROOT)
      and die "rsync failed: $?\n";
  }

  # then rsync them all back out again
  for my $m (@slaves) {
    warn "Sending to $m\n";
    system ("rsync", "-avX", WEBROOT . "/", "$m:" . WEBROOT)
      and die "rsync failed: $?\n";
  }

} elsif ($command eq "md5sum") {

  unless ($xattr_support) {
    warn "MD5 Sums will not be saved (no xattr support)\n";
  }
  my $file  = shift;
  my $sfile = (convert_filename($file))[0];

  my $sum = md5_digest($sfile);
  print "$sfile: $sum\n";

} elsif ($command eq "spider") {

  spider;

} elsif ($command eq "mirror") {

  do_mirror;

} elsif ($command eq "human") {

  foreach (1023,1024,1024**2,1024**3,1024**4,1024**5,1024**6) {
    print human_readable($_) . "\t$_\n";
  }

} elsif ($command eq "omd5") {	# check official md5

  my $file = (convert_filename(shift @ARGV))[0];

  my $hash = official_md5_info($file);
  print "Official MD5: '$hash'\n";

} elsif ($command eq "dltest") {	# download test

  download_url(url => 'http://euler/10Mb', file => './10Mb', verbose => 1) 
    || warn "total failure\n";

} elsif ($command eq "crontask") {

  crontask_remote;
  crontask_local;

} elsif ($command eq "cronlocal") {

  crontask_local;

} else {

  die "Unknown command: $command\n";
}
